{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gdown tqdm albumentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gdown\n",
    "import zipfile\n",
    "import shutil\n",
    "import multiprocessing\n",
    "from zipfile import ZipFile\n",
    "from tqdm import tqdm\n",
    "\n",
    "from download_dataset import main as download_dataset_main\n",
    "from preprocess_data import main as preprocess_data_main\n",
    "from explore_dataset import main as explore_dataset_main\n",
    "from augment_data import main as augment_data_main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_file(url, destination):\n",
    "    \"\"\"Download file from URL to the specified destination using gdown.\"\"\"\n",
    "    print(f\"Downloading file from {url}...\")\n",
    "    try:\n",
    "        gdown.download(url, destination, quiet=False)\n",
    "        print(f\"Downloaded file to {destination}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading file: {e}\")\n",
    "        return False\n",
    "\n",
    "def extract_member(member, zip_path, extract_to):\n",
    "    with ZipFile(zip_path, \"r\") as zip_ref:\n",
    "        zip_ref.extract(member, extract_to)\n",
    "\n",
    "def extract_zip(zip_path, extract_to):\n",
    "    \"\"\"Extract zip file using multiprocessing for speed.\"\"\"\n",
    "    print(f\"Extracting {zip_path} into {extract_to}...\")\n",
    "\n",
    "    with ZipFile(zip_path, \"r\") as zip_ref:\n",
    "        members = zip_ref.namelist()\n",
    "\n",
    "    # Prepare args for multiprocessing\n",
    "    args = [(member, zip_path, extract_to) for member in members]\n",
    "\n",
    "    # Use multiprocessing Pool for faster extraction\n",
    "    with multiprocessing.Pool() as pool:\n",
    "        list(tqdm(\n",
    "            pool.starmap(extract_member, args),\n",
    "            total=len(members),\n",
    "            desc=\"Extracting\"\n",
    "        ))\n",
    "    print(\"Extraction completed.\")\n",
    "\n",
    "def zip_dataset(folder_path, output_path):\n",
    "    \"\"\"Compress the dataset folder into a zip file.\"\"\"\n",
    "    print(f\"Compressing {folder_path} into {output_path}...\")\n",
    "    # Ensure output path doesn't have .zip extension\n",
    "    base_name = os.path.splitext(output_path)[0]\n",
    "    shutil.make_archive(base_name=base_name, format='zip', root_dir=folder_path)\n",
    "    print(\"Dataset compressed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified paths for Kaggle environment\n",
    "dataset_zip = \"/kaggle/working/soict-hackathon-2024_dataset.zip\"\n",
    "data_dir = \"/kaggle/working/data\"\n",
    "dataset_dir = os.path.join(data_dir, \"soict-hackathon-2024_dataset\")\n",
    "\n",
    "# Create data directory if it doesn't exist\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "# Check if dataset directory exists\n",
    "if os.path.exists(dataset_dir):\n",
    "    print(\"Dataset directory already exists. Skipping processing and augmentation.\")\n",
    "else:\n",
    "    # Check if zip file exists\n",
    "    zip_paths = [\n",
    "        os.path.join(\"/kaggle/working\", dataset_zip),\n",
    "        os.path.join(data_dir, dataset_zip),\n",
    "    ]\n",
    "    zip_exists = any(\n",
    "        os.path.exists(path) and os.path.getsize(path) > 0 for path in zip_paths\n",
    "    )\n",
    "\n",
    "    if zip_exists:\n",
    "        # Use existing zip file after validation\n",
    "        zip_path = next(path for path in zip_paths if os.path.exists(path))\n",
    "        print(f\"Found file {zip_path}.\")\n",
    "\n",
    "        # Validate zip file\n",
    "        try:\n",
    "            with ZipFile(zip_path, \"r\") as zf:\n",
    "                bad_file = zf.testzip()\n",
    "                if bad_file:\n",
    "                    raise zipfile.BadZipFile(f\"Bad file found: {bad_file}\")\n",
    "\n",
    "            # If valid, extract it\n",
    "            extract_zip(zip_path, data_dir)\n",
    "            print(\"Dataset ready.\")\n",
    "\n",
    "        except zipfile.BadZipFile:\n",
    "            print(f\"File {zip_path} is not a valid zip file or is corrupted.\")\n",
    "            print(\"Continuing with data processing scripts...\")\n",
    "            # Run processing scripts\n",
    "            print(\"Starting data processing and augmentation...\")\n",
    "            download_dataset_main()\n",
    "            explore_dataset_main()\n",
    "            preprocess_data_main()\n",
    "            augment_data_main()\n",
    "            print(\"Data processing and augmentation completed.\")\n",
    "\n",
    "            # Compress processed dataset\n",
    "            zip_dataset(dataset_dir, os.path.join(data_dir, dataset_zip))\n",
    "            print(\"Dataset compressed.\")\n",
    "    else:\n",
    "        # Skip Google Drive download and proceed with processing\n",
    "        print(\"Skipping Google Drive dataset download.\")\n",
    "        print(\"Running data processing scripts...\")\n",
    "\n",
    "        # Run processing scripts\n",
    "        print(\"Starting data processing and augmentation...\")\n",
    "        download_dataset_main()\n",
    "        explore_dataset_main()\n",
    "        preprocess_data_main()\n",
    "        augment_data_main()\n",
    "        print(\"Data processing and augmentation completed.\")\n",
    "\n",
    "        # Compress processed dataset\n",
    "        zip_dataset(dataset_dir, os.path.join(data_dir, dataset_zip))\n",
    "        print(\"Dataset compressed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vehicle-detection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
